# Example Configuration for MCP-Based Category Classifier
#
# This configuration demonstrates how to use an external MCP (Model Context Protocol)
# service for category classification instead of the built-in Candle/ModernBERT models.
#
# Use cases:
# - Offload classification to a remote service
# - Use custom classification models not supported in-tree
# - Scale classification independently from the router
# - Integrate with existing ML infrastructure

# BERT model for semantic caching and tool selection
bert_model:
  model_id: "models/all-MiniLM-L6-v2"
  threshold: 0.85
  use_cpu: true

# Classifier configuration
classifier:
  # Disable in-tree category classifier (leave model_id empty)
  category_model:
    model_id: ""  # Empty = disabled
    threshold: 0.6
    use_cpu: true
    use_modernbert: false
    category_mapping_path: ""

  # Enable MCP-based category classifier
  mcp_category_model:
    enabled: true                    # Enable MCP classifier
    transport_type: "stdio"          # "stdio" or "http"
    
    # For stdio transport: run a local Python MCP server
    command: "python"
    args: ["-m", "mcp_category_classifier"]
    env:
      PYTHONPATH: "/opt/ml/models"
      MODEL_PATH: "/opt/ml/models/category_classifier"
      LOG_LEVEL: "INFO"
    
    # For http transport: use this instead
    # transport_type: "http"
    # url: "http://localhost:8080/mcp"
    
    tool_name: "classify_text"      # MCP tool name to call
    threshold: 0.6                   # Confidence threshold
    timeout_seconds: 30              # Request timeout

  # PII model configuration (unchanged)
  pii_model:
    model_id: "models/pii_classifier"
    threshold: 0.7
    use_cpu: true
    pii_mapping_path: "models/pii_classifier/pii_type_mapping.json"

# Prompt guard configuration (unchanged)
prompt_guard:
  enabled: true
  model_id: "models/jailbreak_classifier"
  threshold: 0.8
  use_cpu: true
  use_modernbert: true
  jailbreak_mapping_path: "models/jailbreak_classifier/jailbreak_mapping.json"

# Categories for routing queries
categories:
  - name: "math"
    description: "Mathematical problems, equations, calculus, algebra, statistics"
    model_scores:
      - model: "deepseek/deepseek-r1:70b"
        score: 0.95
        use_reasoning: true
      - model: "qwen/qwen3-235b"
        score: 0.90
        use_reasoning: true
    mmlu_categories:
      - "mathematics"
      - "statistics"

  - name: "coding"
    description: "Programming, software development, debugging, algorithms"
    model_scores:
      - model: "deepseek/deepseek-r1-coder:33b"
        score: 0.95
        use_reasoning: true
      - model: "meta/llama3.1-70b"
        score: 0.85
        use_reasoning: false
    mmlu_categories:
      - "computer_science"
      - "engineering"

  - name: "general"
    description: "General knowledge, conversation, misc queries"
    model_scores:
      - model: "meta/llama3.1-70b"
        score: 0.90
        use_reasoning: false
      - model: "qwen/qwen3-235b"
        score: 0.85
        use_reasoning: false

# Default model to use when category can't be determined
default_model: "meta/llama3.1-70b"

# vLLM endpoints configuration
vllm_endpoints:
  - name: "deepseek-endpoint"
    address: "10.0.1.10"
    port: 8000
    models:
      - "deepseek/deepseek-r1:70b"
      - "deepseek/deepseek-r1-coder:33b"
    weight: 100

  - name: "qwen-endpoint"
    address: "10.0.1.11"
    port: 8000
    models:
      - "qwen/qwen3-235b"
    weight: 100

  - name: "llama-endpoint"
    address: "10.0.1.12"
    port: 8000
    models:
      - "meta/llama3.1-70b"
    weight: 100

# Semantic cache configuration (optional)
semantic_cache:
  enabled: true
  backend_type: "in-memory"
  similarity_threshold: 0.90
  max_entries: 1000
  ttl_seconds: 3600
  eviction_policy: "lru"

# Model-specific configuration
model_config:
  "deepseek/deepseek-r1:70b":
    reasoning_family: "deepseek"
    pii_policy:
      allow_by_default: false
      pii_types_allowed: []

  "deepseek/deepseek-r1-coder:33b":
    reasoning_family: "deepseek"
    pii_policy:
      allow_by_default: false
      pii_types_allowed: []

  "qwen/qwen3-235b":
    reasoning_family: "qwen3"
    pii_policy:
      allow_by_default: true

  "meta/llama3.1-70b":
    pii_policy:
      allow_by_default: true

# Reasoning family configurations
reasoning_families:
  deepseek:
    type: "chat_template_kwargs"
    parameter: "thinking"
  qwen3:
    type: "reasoning_effort"
    parameter: "reasoning_effort"
  gpt-oss:
    type: "chat_template_kwargs"
    parameter: "enable_thinking"

# Tools configuration (optional)
tools:
  enabled: false
  top_k: 5
  similarity_threshold: 0.7
  tools_db_path: "config/tools_db.json"
  fallback_to_empty: true

# API configuration
api:
  batch_classification:
    metrics:
      enabled: true
      sample_rate: 1.0

# Observability configuration
observability:
  tracing:
    enabled: false
    provider: "opentelemetry"
    exporter:
      type: "otlp"
      endpoint: "localhost:4317"
      insecure: true
    sampling:
      type: "always_on"
    resource:
      service_name: "semantic-router"
      service_version: "1.0.0"
      deployment_environment: "production"

